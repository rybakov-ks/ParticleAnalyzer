import gradio as gr
import pandas as pd
import csv
import os
from datetime import datetime
from particleanalyzer.core.languages import translations
from particleanalyzer.core.language_context import LanguageContext
from particleanalyzer.core.ParticleAnalyzer import ParticleAnalyzer


def assets_path(name: str):
    return os.path.join(os.path.dirname(__file__), "..", "assets", name)


def determine_language(accept_language: str):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ Accept-Language"""
    if not accept_language:
        return "en"

    lang_code = accept_language.split(",")[0].lower()

    language_mapping = {
        "en-us": "en",
        "en": "en",
        "ru": "ru",
        "zh-cn": "zh-cn",
        "zh-tw": "zh-tw",
    }

    return language_mapping.get(
        lang_code, language_mapping.get(lang_code.split("-")[0], "en")
    )


def get_translation(text):
    lang = LanguageContext.get_language()
    return translations.get(lang, {}).get(text, text)


def translate_chatbot():
    text_chatbot = get_translation(
        """üî¨ **–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ —Å–∏—Å—Ç–µ–º—É –∞–Ω–∞–ª–∏–∑–∞ SEM-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!**
        –Ø ‚Äì –≤–∞—à –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ —Å–∫–∞–Ω–∏—Ä—É—é—â–µ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏–∏. –ì–æ—Ç–æ–≤ –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ –∏ —Ä–∞–∑–º–µ—Ä–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —á–∞—Å—Ç–∏—Ü."""
    )
    return gr.update(value=[[None, text_chatbot]])


def get_columns(scale_value):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ DataFrame –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞"""
    unit = get_translation(ParticleAnalyzer.SCALE_OPTIONS[scale_value]["unit"])
    columns = [
        "‚Ññ",
        f"S [{unit}¬≤]",
        f"P [{unit}]",
        f"D [{unit}]",
        f"D‚Çò‚Çê‚Çì [{unit}]",
        f"D‚Çò·µ¢‚Çô' [{unit}]",
        f"D‚Çò‚Çë‚Çê‚Çô [{unit}]",
        "'Œ∏‚Çò‚Çê‚Çì' [¬∞]",
        "'Œ∏‚Çò·µ¢‚Çô' [¬∞]",
        "e",
        f"I [{get_translation('–µ–¥.')}]" f"{get_translation('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–∏—Ü')}",
    ]
    return pd.DataFrame(columns=columns)


def get_stats_columns():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    columns = [
        get_translation("–ü–∞—Ä–∞–º–µ—Ç—Ä"),
        get_translation("–°—Ä–µ–¥–Ω–µ–µ"),
        get_translation("–ú–µ–¥–∏–∞–Ω–∞"),
        get_translation("–ú–∞–∫—Å–∏–º—É–º"),
        get_translation("–ú–∏–Ω–∏–º—É–º"),
        get_translation("–°–û"),
    ]
    return pd.DataFrame(columns=columns)


def scale_input_visibility(scale_value):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∞—Å—à—Ç–∞–±–Ω—É—é —à–∫–∞–ª—É"""
    is_scaled = ParticleAnalyzer.SCALE_OPTIONS[scale_value]["scale"]
    return (
        gr.update(visible=(is_scaled)),
        gr.update(visible=(is_scaled)),
        gr.update(visible=(not is_scaled)),
        gr.update(value=(get_columns(scale_value))),
        gr.update(visible=(not is_scaled)),
        gr.update(value=(get_columns(scale_value))),
        gr.update(visible=(is_scaled)),
    )


def segment_mode_visibility(segment_mode):
    """–†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–∏—Ü"""
    return gr.update(visible=None if segment_mode else False), gr.update(
        visible=None if segment_mode else False
    )


def select_section(evt: gr.SelectData, output_table):
    """–†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–∏—Ü. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞—Å—Ç–∏—Ü—ã"""
    if 0 <= evt.index < len(output_table):
        return output_table.iloc[[evt.index]], gr.update(visible=True)
    else:
        return empty_df_ParticleCharacteristics, gr.update(visible=False)


def sahi_mode_visibility(sahi_mode):
    """–†–µ–∂–∏–º SAHI"""
    return (
        gr.update(visible=sahi_mode),
        gr.update(visible=sahi_mode),
        gr.update(visible=not sahi_mode),
        gr.update(visible=not sahi_mode),
        gr.update(value=False if sahi_mode else None),
    )


def reset_interface(scale_value):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±—Ä–æ—Å–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    return (
        {"background": None, "layers": [], "composite": None},  # –û—á–∏—â–∞–µ–º im
        None,  # –û—á–∏—â–∞–µ–º output_image
        None,  # –û—á–∏—â–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        None,  # –û—á–∏—â–∞–µ–º input_image
        None,  # –û—á–∏—â–∞–µ–º output_image2
        gr.update(visible=False),  # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É AnnotatedImage_row
        gr.update(visible=False),  # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É output_table_image2_row
        [(None, None)],  # –û—á–∏—â–∞–µ–º chatbot
        gr.update(visible=False),  # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É results_row
    )


def reset_interface2(scale_value):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±—Ä–æ—Å–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    return (
        None,  # –û—á–∏—â–∞–µ–º output_image
        None,  # –û—á–∏—â–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        None,  # –û—á–∏—â–∞–µ–º input_image
        None,  # –û—á–∏—â–∞–µ–º output_image2
        gr.update(visible=False),  # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É AnnotatedImage_row
        gr.update(visible=False),  # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É output_table_image2_row
        [(None, None)],  # –û—á–∏—â–∞–µ–º chatbot
        gr.update(visible=False),  # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É results_row
    )


def scale_input_unit_measurement(scale_selector, request: gr.Request):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–µ–π–±–ª–∞ –¥–ª—è –¥–ª–∏–Ω—ã —à–∫–∞–ª—ã –ø—Ä–∏–±–æ—Ä–∞"""
    lang = determine_language(request.headers.get("Accept-Language", ""))
    LanguageContext.set_language(lang)
    unit = ParticleAnalyzer.SCALE_OPTIONS[scale_selector]["unit"]
    label = get_translation(f"–î–ª–∏–Ω–∞ —à–∫–∞–ª—ã –≤ {unit}")
    return gr.update(label=label)


def save_data_to_csv(
    data_table: pd.DataFrame, data_table2: pd.DataFrame, output_dir: str = "output"
):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏—Ü –≤ CSV —Ñ–∞–π–ª—ã"""
    os.makedirs(output_dir, exist_ok=True)

    particle_path = os.path.join(output_dir, "particle_characteristics.csv")
    stats_path = os.path.join(output_dir, "particle_statistics.csv")

    data_table.to_csv(particle_path, index=False, encoding="utf-8-sig")
    data_table2.to_csv(stats_path, index=False, encoding="utf-8-sig")

    return [particle_path, stats_path]


def log_analytics(
    confidence_threshold: float,
    confidence_iou: float,
    model_name: str,
    feedback: str,
    output_dir: str = "output",
):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ —Ä–∞–±–æ—Ç–µ –º–æ–¥–µ–ª–∏"""
    os.makedirs(output_dir, exist_ok=True)

    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
    files = [
        f
        for f in os.listdir(output_dir)
        if not f.endswith(".csv") and os.path.isfile(os.path.join(output_dir, f))
    ]

    latest_file = (
        max(files, key=lambda x: os.path.getmtime(os.path.join(output_dir, x)))
        if files
        else "N/A"
    )

    analytic_path = os.path.join(output_dir, "analytics.csv")
    file_exists = os.path.isfile(analytic_path)

    with open(analytic_path, mode="a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(
                ["Timestamp", "Model", "Confidence", "IoU", "Feedback", "ProcessedFile"]
            )
        writer.writerow(
            [
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                model_name,
                confidence_threshold,
                confidence_iou,
                feedback,
                latest_file,
            ]
        )

    return gr.update(visible=False), gr.update(selected=1)


def chatbot_visibility():
    return gr.update(visible=True)


empty_df_ParticleCharacteristics = get_columns("Pixels").fillna("")
empty_df_ParticleStatistics = get_stats_columns()

toggleTheme = """
() => {
    function toggleTheme() {
        const isDark = document.body.classList.toggle('dark');
        localStorage.setItem('gradioDarkMode', isDark);
    }

    function getSystemTheme() {
        return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    }

    const toggle = document.getElementById('darkModeToggle');
    if (!toggle) return;

    const savedTheme = localStorage.getItem('gradioDarkMode');
    const systemTheme = getSystemTheme();

    const isDark = savedTheme !== null 
        ? savedTheme === 'true' 
        : systemTheme === 'dark';

    if (isDark) {
        document.body.classList.add('dark');
        toggle.checked = true;
    } else {
        document.body.classList.remove('dark');
        toggle.checked = false;
    }

    toggle.addEventListener('change', toggleTheme);

    if (savedTheme === null) {
        const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');
        const handleSystemThemeChange = (e) => {
            if (e.matches) {
                document.body.classList.add('dark');
                toggle.checked = true;
            } else {
                document.body.classList.remove('dark');
                toggle.checked = false;
            }
        };

        handleSystemThemeChange(mediaQuery);

        mediaQuery.addEventListener('change', handleSystemThemeChange);
    }
}

"""
